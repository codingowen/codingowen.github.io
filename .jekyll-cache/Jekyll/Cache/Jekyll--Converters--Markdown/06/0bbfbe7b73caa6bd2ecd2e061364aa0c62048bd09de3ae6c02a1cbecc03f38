I"õ(<p>In this project, weâ€™re going to learn about the Self-Attention mechanism used in the recent Transformer architecture.</p>

<p>This post will begin with an intuitive explanation of how the attention mechanism works, followed by a code-along section where we implement the attention mechanism for calculating the attention scores of input text sentences.</p>

<p>Hereâ€™s the structure for this project post:</p>

<p>1. Intuitive Explanation of Attention <br />
1.1 Embeddings &amp; Context <br />
1.2 Similarity <br />
1.3 Attention <br />
1.4 Keys &amp; Queries Matrices <br />
1.5 Values Matrix <br />
1.6 Self &amp; Multi-Headed Attention <br />
2. Code Implementation of Self-Attention Mechanism <br />
3. References</p>

<h2 id="1-intuitive-explanation-of-attention">1. Intuitive Explanation of Attention</h2>

<p>In this chapter, we will go through some intuitive explanations of Attention and its core components. Weâ€™ll cover the role of embedding, context and similarity measures, before going deeper into defining Attention, and the K-Q-V components.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="11-embeddings-and-context">1.1 Embeddings and Context</h3>

<p>Letâ€™s begin with an input sentence. We need to represent this sentence mathematically, so that it can work with our various machine learning models.</p>

<p>To do this, we will represent each word (or token) in the sentence as a vector.</p>

<p>As we know, a vector in 3D space might look like [1, 2, 3], representing coordinates along three dimensions. In the context of text, when we embed a word as a vector, we convert it into a similar ordered list of numbers.</p>

<p>Each dimension of the embedding vector represents different aspects of meaning, or relationships between the words.</p>

<p>Embedding vectors are used to capture the semantics of our input text such that similar inputs are close to each other in the embedding space.</p>

<p>Hereâ€™s a simple but clarifying example:</p>

<p>Letâ€™s say we have a bunch of words representing fruits, such as â€˜Orangeâ€™, â€˜Bananaâ€™, etc. We also have a bunch of words representing technology products, such as â€˜Androidâ€™, â€˜Microsoftâ€™, etc.</p>

<p>Now, we embed the input words to a 2D embedding space:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_1.png" height="auto" width="55%" />
</div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_2.png" height="auto" width="75%" />
</div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<p>Now, the question is, where would you put the word â€˜Appleâ€™? It could both refer to the fruit, or the tech company.</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_3.png" height="auto" width="75%" />
</div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<p>Well, to figure this out, naturally, we would look at the context of the whole input sentence.</p>

<p>For example, given an input sentence â€˜Please buy an Apple and an Orangeâ€™, then we know for sure â€˜Appleâ€™ refers to the fruit. On the other hand, if the sentence was â€˜Apple unveiled their new phoneâ€™, then we would know that â€˜Appleâ€™ refers to the tech company.</p>

<p>Then, weâ€™ll embed the word â€˜Appleâ€™ closer to the relevant context-providing word!</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_4.png" height="auto" width="100%" />
</div>

<p>So, each word needs to be given context, which is derived from the neighbouring words. So in the case of the first sentence involving apples and oranges, weâ€™d move the word â€˜Appleâ€™ closer to the word â€˜Orangeâ€™ in the embedding space, where the word â€˜Appleâ€™ and â€˜Orangeâ€™ contribute to each otherâ€™s â€˜fruitâ€™ context. The mechanism is the same for the second sentence.</p>

<p>In the case of the Attention mechanism, context refers to the information gathered from other words in the sentence, that influences the representation of a given word. To be clear, â€˜contextâ€™ is not a single variable or metric, but rather an emergent property of how tokens are related and influence each other dynamically based on meaning and position.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="12-similarity">1.2 Similarity</h3>

<p>Now, hopefully we see why understanding the context of the input text helps us decide on the best way to represent the relationships between the input words (via their embeddings).</p>

<p>In the earlier example, we saw how the word â€˜Appleâ€™ moves closer to â€˜Orangeâ€™ in the embedding space when our input text refers to fruits. This means that words with similar meanings are represented as vectors that are close together in the embedding space, while unrelated words are farther apart.</p>

<p>Okay, then the next question naturally becomes, how do we know that weâ€™ve embedded our words correctly? Meaning, how do we know when similar words like â€˜Appleâ€™ and â€˜Orangeâ€™ have been correctly embedded to be vectors that lie closer to each other?</p>

<p>To do that, weâ€™ll need a way to measure the similarity of the embedded vectors!</p>

<p>One common way we measure similarity is by using the <u>Dot Product</u> of the embedding vectors. Weâ€™ll illustrate how the Dot Product works:</p>

<p>Firstly, recall that in the embedding space, each dimension represents some relationship or meaning within the input text. For example, given our earlier input words (â€˜Orangeâ€™, â€˜Androidâ€™, etc.), our 2D embedding space could include one dimension representing the â€˜Fruitâ€™ characteristic and one dimension representing the â€˜Techâ€™ characteristic of our input text, like so:</p>

<p>Letâ€™s say we have the following setup:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_5.png" height="auto" width="80%" />
</div>

<p>Then, the dot product computation would be like so:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_6.png" height="auto" width="100%" />
</div>

<p>We can see that words that are similar, like â€˜Appleâ€™ and â€˜Orangeâ€™, will have a greater dot product value, while words that are dissimilar will have a smaller dot product value.</p>

<p>Additionally, we can see how the dot product of the â€˜Orangeâ€™ and â€˜Androidâ€™ embedding vectors would be 0 (due to their orthogonality). This ensures that words that are purely about fruits and words that are purely about tech have no dot product similarity.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<p>However, the There are other measures of similarity like Cosine Similarity, but the Attention Mechanism usually uses something called <u>Scaled Dot Product Similarity</u>.</p>

<p>The Scaled Dot Product is simply the Dot Product divided by the square root of the length of the vector.</p>

<ul>
  <li>
    <p>explain scaled dot product</p>
  </li>
  <li>
    <p>however, these embeddings arenâ€™t optimal. explain why</p>
  </li>
  <li>
    <p>we can actually stretch and compress certain dimensions using this matrix to make our embeddings more optimal.</p>
  </li>
  <li>
    <p>but, thatâ€™s not the end of the story. Aside from stretching/compressing dimensions, we may also want to combine the effect of some dimensions</p>
  </li>
  <li>
    <p>maybe the embedding was a little too extreme, and we want to move them closer because we think theyâ€™re similar people.</p>
  </li>
</ul>
:ET