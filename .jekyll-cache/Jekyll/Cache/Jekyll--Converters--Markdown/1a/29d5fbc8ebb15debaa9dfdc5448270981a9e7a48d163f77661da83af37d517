I"Å<p>In this post, I will attempt to rationalize my geometric intuition for how Self-Attention works in Large Language Models.</p>
:ET