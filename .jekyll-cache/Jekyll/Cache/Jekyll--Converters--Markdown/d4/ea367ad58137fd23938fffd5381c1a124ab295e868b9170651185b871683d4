I"ß<p>In this project, weâ€™re going to build a simple Long Short Term Memory (LSTM)-based recurrent model from scratch, using NumPy.</p>

<p>Weâ€™ll employ the LSTM model on the same task as our previous RNN model, and find out which model produces better song lyrics.</p>

<p>Given the foundational knowledge on RNNs established in previous projects, weâ€™ll move relatively quickly into the theory for LSTMs.</p>

<p>Hereâ€™s the structure for this project post:</p>

<p>1. LSTM Overview</p>

<p>2. Describing the LSTM mathematically</p>

<p>2.1 Forward Propagation</p>

<p>2.2 Backward Propagation</p>

<p>2.2.1 Deriving Components In Backpropagation Through Time</p>

<p>2.3 How LSTMs Solve Vanishing/Exploding Gradients</p>

<p>3. Code Implementation &amp; Results</p>

<h2 id="1-lstm-overview">1. LSTM Overview</h2>

<p>In this section, weâ€™ll cover the architecture of the LSTM cell, and examine how it improves upon the basic RNN model we learned in the previous project post. In order to do that, weâ€™ll have a super quick review of RNNs, and modify our visual perspective of RNNs just a little.</p>

<p>Recall that RNNs are a special type of neural network consisting of recursive computational loops that span adjacent time steps. We visualized them unfolding like this:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/rnn_2.png" height="auto" width="75%" />
</div>

<p>Also, recall that our RNN has biases and activation functions. We describe the net</p>

<p>Now, to prepare for learning about LSTMs, weâ€™ll change our visual understanding of RNNs just a little. Now, weâ€™ll need to see RNNs as forming a chain of repeating modules, with a very simple structure, such as a single $\text{tanh}$ layer:</p>

:ET