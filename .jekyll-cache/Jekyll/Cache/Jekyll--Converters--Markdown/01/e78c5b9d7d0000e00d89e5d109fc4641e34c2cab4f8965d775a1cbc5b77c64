I"f'<p>In this project, we’re going to build a simple neural network from scratch, using NumPy.</p>

<p>We’ll build a shallow 2-layer neural network (one hidden and one output). This neural network will be trained on data from the MNIST handwritten digit dataset, and classify input images to digits.</p>

<p>The MNIST dataset consists of 28x28 grayscale images of handwritten digits. Each image is labelled with the digit it belongs to.</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/mnist.png" height="auto" width="40%" />
</div>

<p>Here’s the structure for this project post:</p>

<p>1. Neural Network Overview</p>

<p>2. Describing the Neural Network Mathematically</p>

<p>2.1 Forward Propagation</p>

<p>2.2 Backward Propagation</p>

<p>3. Code Implementation</p>

<h2 id="1-neural-network-overview">1. Neural Network Overview</h2>

<p>Our Neural Network will have three layers in total - an input layer, a hidden layer and an output layer.</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/nn_from_scratch.png" height="auto" width="60%" />
</div>

<p>The input layer has 784 nodes, which corresponds to the total number of pixels in each 28x28 input image from the MNIST dataset. Each pixel is described by a value between [0,255], which represents pixel intensity (white being 255, black being 0).</p>

<p>Next, to keep things simple, we’ll give the hidden layer 10 nodes. The value of these nodes is first calculated based on the weights and biases applied to the 784 nodes from the input layer, followed by a ReLU activation function.</p>

<p>Finally, the output layer will have 10 nodes, which corresponds to each output class. We have 10 possible digit values (0 to 9), so we have 10 output classes. The value of these nodes are first calculated based on the weights and biases applied to the 10 nodes from the hidden layer, followed by a Softmax activation function.</p>

<p>Then, the output layer will give us a column of 10 probability values, containing a value between [0,1] for each class.</p>

<p>We’ll then classify the image based on the digit class with the highest probability value.</p>

<h2 id="2-describing-the-neural-network-mathematically">2. Describing the Neural Network Mathematically</h2>

<p>In this section, we will formalize our neural network mathematically, so that we can reproduce relevant variables in code later.</p>

<p><b><u>We'll begin by understanding the dimensionality of our input data.</u></b></p>

<p>Conventionally, our input data would stack the information for each image as rows of a matrix. However, because we will be doing matrix multiplication with weight vectors, we’ll transpose the conventional matrix to obtain our matrix $X$, which has image data as columns instead. Thus, matrix $X$ would have columns of height $784$, with $m$ total columns (where our input dataset has $m$ images, for example).</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/nn_from_scratch2.png" height="auto" width="60%" />
</div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext> </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak"> </span></span><span class="mspace newline"></span></span></span></span></p>

<p><b><u>Next, we'll look at the weights and biases between our neural network layers.</u></b></p>

<p>Let’s index our layers like so: the input layer is layer 0, hidden layer is layer 1 and output layer is layer 2.</p>

<p>Between each pair of layers is a set of connections between every node in the previous node and every node in the following one. When values from one layer get passed to the next layer, there is a weight applied to each node value from the original layer, followed by a bias term added to the weighted node value. Finally, there will be an activation function applied to the weighted and biased node value, to add non-linearity to the output.</p>

<p>Mathematically, we would describe it like this:</p>

<p>Let’s say we’re going from layer 0 (784 nodes) to layer 1 (10 nodes). Each node $h_i$ in layer 1 is computed as:</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><menclose notation="box"><mstyle scriptlevel="0" displaystyle="false"><mstyle scriptlevel="0" displaystyle="false"><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mstyle scriptlevel="0" displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>784</mn></munderover><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>x</mi><mi>j</mi></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mstyle></mrow></mstyle></mstyle></mstyle></menclose></mrow><annotation encoding="application/x-tex"> 
\boxed{h_i = f( \displaystyle\sum_{j=1}^{784} w_{ij} x_j + b)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.89489em;vertical-align:-1.7537769999999997em;"></span><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.1411130000000007em;"><span style="top:-5.89489em;"><span class="pstrut" style="height:5.89489em;"></span><span class="boxpad"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8011130000000004em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">7</span><span class="mord mtight">8</span><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span></span></span><span style="top:-4.141113000000001em;"><span class="pstrut" style="height:5.89489em;"></span><span class="stretchy fbox" style="height:3.89489em;border-style:solid;border-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.7537769999999997em;"><span></span></span></span></span></span></span></span></span></span></p>

<p>Where:</p>
<ul>
  <li>$x_j$ is the input value from originating node $j$</li>
  <li>$w_ij$ is the weight connecting input node $j$ to node $i$</li>
  <li>$b_i$ is the bias applied to node $i$</li>
  <li>$f(\cdot)$ is the activation function (eg. ReLU, sigmoid, tanh)</li>
</ul>

<p>Thus, we should</p>
:ET