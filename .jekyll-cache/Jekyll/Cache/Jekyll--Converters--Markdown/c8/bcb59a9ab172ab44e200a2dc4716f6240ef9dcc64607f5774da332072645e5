I"Ñg<p>In this project, weâ€™re going to learn about the Self-Attention mechanism used in the recent Transformer architecture.</p>

<p>This post will begin with an intuitive explanation of how the attention mechanism works, followed by a code-along section where we implement the attention mechanism for calculating the attention scores of input text sentences.</p>

<p>Hereâ€™s the structure for this project post:</p>

<p>1. Intuitive Explanation of Attention <br />
1.1 Embeddings &amp; Context <br />
1.2 Similarity <br />
1.3 Attention <br />
1.4 Keys &amp; Queries Matrices <br />
1.5 Values Matrix <br />
1.6 Self &amp; Multi-Headed Attention <br />
2. Code Implementation of Self-Attention Mechanism <br />
3. References</p>

<h2 id="1-intuitive-explanation-of-attention">1. Intuitive Explanation of Attention</h2>

<p>In this chapter, we will go through some intuitive explanations of Attention and its core components. Weâ€™ll cover the role of embedding, context and similarity measures, before going deeper into defining Attention, and the K-Q-V components.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="11-embeddings-and-context">1.1 Embeddings and Context</h3>

<p>Letâ€™s begin with an input sentence. We need to represent this sentence mathematically, so that it can work with our various machine learning models.</p>

<p>To do this, we will represent each word (or token) in the sentence as a vector.</p>

<p>As we know, a vector in 3D space might look like [1, 2, 3], representing coordinates along three dimensions. In the context of text, when we embed a word as a vector, we convert it into a similar ordered list of numbers.</p>

<p>Each dimension of the embedding vector represents different aspects of meaning, or relationships between the words.</p>

<p>Embedding vectors are used to capture the semantics of our input text such that similar inputs are close to each other in the embedding space.</p>

<p>Hereâ€™s a simple but clarifying example:</p>

<p>Letâ€™s say we have a bunch of words representing fruits, such as â€˜Orangeâ€™, â€˜Bananaâ€™, etc. We also have a bunch of words representing technology products, such as â€˜Androidâ€™, â€˜Microsoftâ€™, etc.</p>

<p>Now, we embed the input words to a 2D embedding space:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_1.png" height="auto" width="55%" />
</div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_2.png" height="auto" width="75%" />
</div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<p>Now, the question is, where would you put the word â€˜Appleâ€™? It could both refer to the fruit, or the tech company.</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_3.png" height="auto" width="75%" />
</div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<p>Well, to figure this out, naturally, we would look at the context of the whole input sentence.</p>

<p>For example, given an input sentence â€˜Please buy an Apple and an Orangeâ€™, then we know for sure â€˜Appleâ€™ refers to the fruit. On the other hand, if the sentence was â€˜Apple unveiled their new phoneâ€™, then we would know that â€˜Appleâ€™ refers to the tech company.</p>

<p>Then, weâ€™ll embed the word â€˜Appleâ€™ closer to the relevant context-providing word!</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_4.png" height="auto" width="100%" />
</div>

<p>So, each word needs to be given context, which is derived from the neighbouring words. So in the case of the first sentence involving apples and oranges, weâ€™d move the word â€˜Appleâ€™ closer to the word â€˜Orangeâ€™ in the embedding space, where the word â€˜Appleâ€™ and â€˜Orangeâ€™ contribute to each otherâ€™s â€˜fruitâ€™ context. The mechanism is the same for the second sentence.</p>

<p>In the case of the Attention mechanism, context refers to the information gathered from other words in the sentence, that influences the representation of a given word. To be clear, â€˜contextâ€™ is not a single variable or metric, but rather an emergent property of how tokens are related and influence each other dynamically based on meaning and position.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="12-similarity">1.2 Similarity</h3>

<p>Now, hopefully we see why understanding the context of the input text helps us decide on the best way to represent the relationships between the input words (via their embeddings).</p>

<p>In the earlier example, we saw how the word â€˜Appleâ€™ moves closer to â€˜Orangeâ€™ in the embedding space when our input text refers to fruits. This means that words with similar meanings are represented as vectors that are close together in the embedding space, while unrelated words are farther apart.</p>

<p>Okay, then the next question naturally becomes, how do we know that weâ€™ve embedded our words correctly? Meaning, how do we know when similar words like â€˜Appleâ€™ and â€˜Orangeâ€™ have been correctly embedded to be vectors that lie closer to each other?</p>

<p>To do that, weâ€™ll need a way to measure the similarity of the embedded vectors!</p>

<p>One common way we measure similarity is by using the <b>Dot Product</b> of the embedding vectors. Weâ€™ll illustrate how the Dot Product works:</p>

<p>Firstly, recall that in the embedding space, each dimension represents some relationship or meaning within the input text. For example, given our earlier input words (â€˜Orangeâ€™, â€˜Androidâ€™, etc.), our 2D embedding space could include one dimension representing the â€˜Fruitâ€™ characteristic and one dimension representing the â€˜Techâ€™ characteristic of our input text, like so:</p>

<p>Letâ€™s say we have the following setup:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_5.png" height="auto" width="80%" />
</div>

<p>Then, the dot product computation would be like so:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_6.png" height="auto" width="100%" />
</div>

<p>We can see that words that are similar, like â€˜Appleâ€™ and â€˜Orangeâ€™, will have a greater dot product value, while words that are dissimilar will have a smaller dot product value.</p>

<p>Additionally, we can see how the dot product of the â€˜Orangeâ€™ and â€˜Androidâ€™ embedding vectors would be 0 (due to their orthogonality). This ensures that words that are purely about fruits and words that are purely about tech have no dot product similarity.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<p>However, the Dot Product is not a perfect measure of similarity, because it can be influenced by the magnitude of vectors involved. There are other measures of similarity like Cosine Similarity, but the original Attention Mechanism in the 2017 paper uses something called <b>Scaled Dot Product Similarity</b>.</p>

<p>The Scaled Dot Product is simply the Dot Product divided by the square root of the length of the vector.</p>

<p>Okay, to summarize what weâ€™ve learnt so far:</p>
<ul>
  <li>We need to embed words as vectors in an embedding space.</li>
  <li>To determine the optimal embedding location for each word, we need to understand the context of each sentence.</li>
  <li>To measure how well weâ€™ve embedded similar words together, we will measure the similarity of their embedding vectors, using tools like the Dot Product, Cosine Similarity, or Scaled Dot Product Similarity.</li>
</ul>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="13-attention">1.3 Attention</h3>

<p>In this section, weâ€™ll see how to apply the â€˜Attentionâ€™ step to our token embeddings. Attention represents how much focus we should give to the other words in an input sentence when processing each word in it.</p>

<p>Letâ€™s say we have an 3D embedding for the words â€˜Orangeâ€™, â€˜Appleâ€™ and â€˜Phoneâ€™, along with grammatical words like â€˜andâ€™ and â€˜anâ€™, like so:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_7.png" height="auto" width="80%" />
</div>

<p>Letâ€™s find the similarity value between each pair of words (i.e. each pair of embedded vectors). For simplicity of computation, weâ€™ll use Cosine Similarity, but you can imagine that any other similarity measure would work too.</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_8.png" height="auto" width="100%" />
</div>

<p>This pairwise similarity measure will help us understand how the existing words in an input sentence will influence each other word.</p>

<p>Take for example, given the input sentence â€˜An Apple And An Orangeâ€™, and the pairwise similarity table:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_9.png" height="auto" width="100%" />
</div>

<p>Another example - what if our input sentence was â€˜An Apple Phoneâ€™?</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_10.png" height="auto" width="100%" />
</div>

<p>So now, we have a way to mathematically describe how much each other word in an input sentence influences, or provides context to each word:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_11.png" height="auto" width="80%" />
</div>

<p>However, we want to prevent the magnitude of the coefficients from growing out of control, and handle negative coefficients, so weâ€™ll apply the SoftMax function to these values:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_12.png" height="auto" width="100%" />
</div>

<p>However, just to be a little more mathematically rigorous, we need to take note that the SoftMax function also assigns any 0 value a real positive value now. For example, recall the pairwise similarity score for the word â€˜Orangeâ€™:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_13.png" height="auto" width="80%" />
</div>

<p>However, for our example, weâ€™ll simply acknowledge that they hold relatively smaller influence and weâ€™ll ignore these extra terms for simplicityâ€™s sake.</p>

<p>Letâ€™s go back to our pairwise similarity equations. Given the equations, we can say that after taking into account the original embedding of the word â€˜Appleâ€™ and the pairwise similarity values with other words, we will account for the influence of the other words and adjust our embedding.</p>

<p>For example, given the input sentence â€˜An Apple And An Orangeâ€™, weâ€™ll take 43% of the original â€˜Appleâ€™ embedding values and replace it with the â€˜Orangeâ€™ embedding values instead. Geometrically, this means weâ€™re taking the line between â€˜Appleâ€™ and â€˜Orangeâ€™, and moving it 43% of the way closer:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_14.png" height="auto" width="100%" />
</div>

<p>So, we can see that the embedding of the word â€˜Appleâ€™ has improved, after taking into account the context of the other words. We can imagine that for many words, across many rounds of iterations, our embeddings will improve significantly and similar words will be optimally grouped together.</p>

<p>Thus, we can see the benefit of paying â€˜Attentionâ€™ to the other important context-providing words in a sentence. This is the essence of the attention mechanism.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="14-keys-and-queries-matrices">1.4 Keys and Queries Matrices</h3>

<p>In the previous section, we learnt how applying the â€˜Attention stepâ€™ (where we compute the pairwise similarity scores and modify our original embedding) allows us to improve the original word embeddings.</p>

<p>Recall that we have the embedding visualization for the word â€˜Appleâ€™, along with â€˜Orangeâ€™ and â€˜Phoneâ€™.</p>

<p>What if our embedding was slightly different? Letâ€™s see three examples:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_15.png" height="auto" width="100%" />
</div>

<p>Which embedding space would be the best for applying our â€˜Attentionâ€™ step to?</p>

<p>We can see that the middle embedding is not ideal, because our â€˜Appleâ€™ embeddings will not be very distinctly separated even after calculating the pairwise similarity and applying the â€˜Attentionâ€™ step. On the other hand, the rightmost embedding is ideal because it makes the separation between our â€˜Appleâ€™ embeddings even more distinct:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_16.png" height="auto" width="100%" />
</div>

<p>Hence, the point is clear that some embedding spaces are better than others. Now, how can we attain the rightmost embedding space from our original embedding space?</p>

<p>Recall that matrices represent combinations of linear transformations. Could we use matrices to linearly transform our original embedding space?</p>

<p>Hereâ€™s where the Keys and Queries matrices come in.</p>

<p>So, letâ€™s say you have an input sentence containing the words â€˜Orangeâ€™ and â€˜Phoneâ€™. As weâ€™ve seen, we need to embed them as vectors in an embedding space.</p>

<p>Now, letâ€™s say weâ€™re trying to figure out how much attention the â€˜Orangeâ€™ vector should give to the â€˜Phoneâ€™ vector. In that case, the â€˜Orangeâ€™ vector is the â€˜Queryâ€™, while the â€˜Phoneâ€™ vector is the â€˜Keyâ€™ (the comparison point used for computing similarity).</p>

<p>Originally, without the Keys and Queries weight matrices, we would directly compute the pairwise similarity of the raw â€˜Orangeâ€™ and â€˜Phoneâ€™ vector embeddings and achieve this original embedding space:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_17.png" height="auto" width="90%" />
</div>

<p>However, we now know that this original embedding space may not be optimal for computing attention. Instead, we apply linear transformations via the Keys and Queries weight matrices to project the embeddings into a space where similarity is more meaningful for attention:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_18.png" height="auto" width="90%" />
</div>

<p>So in summary, the Keys and Queries weight matrices are a way to transform our original embeddings into a more optimal one for calculating pairwise similarities. The resulting matrices containing our linearly transformed token embeddings are called the Keys Matrix and the Queries Matrix.</p>

<p>One might also ask, why do we need both two matrices for the intended linear transformation? Using different learned matrices allows the modelt to optimize the embedding/representation separately for queries and keys. This means the model can optimize what the â€œqueriesâ€ are asking for, and what â€œkeysâ€ are offering as information. In effect, using two matrices instead of one allows the model to learn fine-grained attention patterns better.</p>

<p>To summarize our learnings formally, what weâ€™ve found with the Keys (K) and Queries (Q) matrices is:</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><menclose notation="box"><mstyle scriptlevel="0" displaystyle="false"><mstyle scriptlevel="0" displaystyle="false"><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>A</mi><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo></mrow></mstyle></mstyle></mstyle></menclose></mrow><annotation encoding="application/x-tex"> 
\boxed{A = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.128331em;vertical-align:-1.27em;"></span><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8583310000000002em;"><span style="top:-5.128331em;"><span class="pstrut" style="height:5.128331em;"></span><span class="boxpad"><span class="mord"><span class="mord"><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183309999999999em;"><span style="top:-2.25278em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.18278000000000005em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span><span style="top:-3.858331em;"><span class="pstrut" style="height:5.128331em;"></span><span class="stretchy fbox" style="height:3.128331em;border-style:solid;border-width:0.04em;"></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:1.27em;"><span></span></span></span></span></span></span></span></span></span></p>

<p>Whereby:</p>
<ul>
  <li>$\text{A}$ is the resulting attention weight matrix, containing values between 0 and 1, representing how much focus/attention each token should give to each other token</li>
  <li>$\text{Q}$ is the Query Matrix, representing the transformed embedding of the queries for each token, where $Q = XW_Q$</li>
  <li>$\text{K}$ is the Key Matrix, representing the transformed embedding of the keys for each token, where $K = XW_K$</li>
  <li>$d_k$ is the dimensionality of each key vector</li>
</ul>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Â </mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">Â </span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="14-values-matrix">1.4 Values Matrix</h3>

<p>So weâ€™ve learned that the Keys and Queries matrices help us find the ideal embedding space to find pairwise similarities between our embedding vectors. The resulting computation allows us to find the attention scores of each token - so the Keys and Queries matrices tell us how much focus each token should get.</p>

<p>However, this does not mean that the resultant embedding space from the Keys and Queries transformations is an ideal one for deciding what actual information should be used, or what the best choice is for the next word in the generated output sentence.</p>

<p>Instead, we need another matrix, called the Values weight matrix, to produce a linear transformation, such that we have an optimal embedding space for conveying information. That way, we can obtain the values $V = X W_V$.</p>

<p>Then, weâ€™ll use the Attention Weights obtained from the Keys and Queries matrices to optimize the embedding in our Values matrix embedding space. Letâ€™s see how this words visually:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_19.png" height="auto" width="90%" />
</div>

<p>Okay, now we have a lot to juggle, so letâ€™s simplify these three matrices and their roles:</p>
<ul>
  <li>Query (Q) asks: if I am (this)</li>
</ul>
:ET