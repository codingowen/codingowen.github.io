I"èÿ<p>In this project, we‚Äôre going to implement the Self-Attention mechanism used in the Transformer architecture.</p>

<p>This post will begin with a short recap of how the attention mechanism works, followed by a code-along section where we implement the attention mechanism for calculating the attention scores of input text sentences.</p>

<p>Here‚Äôs the structure for this project post:</p>

<p>1. Quick recap on how Self-Attention works <br />
2. Code Implementation of Self-Attention Mechanism <br />
3. References</p>

<p>*Additionally, I‚Äôd like to mention that I did a detailed technical writeup on how the Self-Attention mechanism works geometrically. <a href="https://codingowen.github.io/blog/2025/02/27/self_attention_intuition/" target="_blank">Please check it out first to get a good geometric intuition for the roles of the K,Q,V matrices.</a></p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="1-quick-recap-on-how-self-attention-works">1. Quick Recap on how Self-Attention works</h3>

<p>The (scaled dot-product) Self-Attention mechanism is defined mathematically as:</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><menclose notation="box"><mstyle scriptlevel="0" displaystyle="false"><mstyle scriptlevel="0" displaystyle="false"><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>K</mi><mo separator="true">,</mo><mi>Q</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow></mstyle></mstyle></mstyle></menclose></mrow><annotation encoding="application/x-tex"> 
\boxed{\text{Attention}(K,Q,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.128331em;vertical-align:-1.27em;"></span><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8583310000000002em;"><span style="top:-5.128331em;"><span class="pstrut" style="height:5.128331em;"></span><span class="boxpad"><span class="mord"><span class="mord"><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183309999999999em;"><span style="top:-2.25278em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.18278000000000005em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span></span><span style="top:-3.858331em;"><span class="pstrut" style="height:5.128331em;"></span><span class="stretchy fbox" style="height:3.128331em;border-style:solid;border-width:0.04em;"></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.27em;"><span></span></span></span></span></span></span></span></span></span></p>

<p>Visually, the operations that happen within the scaled dot-product self-attention formula are like so:</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_21.png" height="auto" width="50%" />
</div>

<p>We learned in the other technical writeup that the Keys and Queries matrices help us find the ideal embedding space to find pairwise similarities between our embedding vectors. The resulting computation allows us to find the attention scores of each token - so the Keys and Queries matrices tell us how much focus each token should get.</p>

<p>Then, we‚Äôll use the Attention Weights obtained from the Keys and Queries matrices to optimize the embedding in our Values matrix embedding space, which gives us context-aware, optimized embeddings of our tokens.</p>

<p>Here‚Äôs a visualization of how the context-aware, optimized token embeddings might be obtained after passing our original token embeddings into the Self-Attention mechanism.</p>

<div class="md-image-container">
    <img class="post-image" src="/assets/images/attention_20.png" height="auto" width="100%" />
</div>

<p>Downstream, this attention output (the contextualized token embeddings) might be passed into a neural network for further refining, then each final token embedding might be mapped into a probability distribution over the entire vocabulary. This allows us to do next-word prediction, and create useful sentences like GPT does.</p>

<p>Alright, now we‚Äôre ready to implement the attention mechanism in code.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="2-code-implementation-of-self-attention-mechanism">2. Code Implementation of Self-Attention Mechanism</h3>

<p>This code implementation will reflect my attempt to modify Sebastian Raschka‚Äôs <a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html" target="_blank">implementation.</a></p>

<h3 id="embedding-an-input-sentence">Embedding an Input Sentence</h3>

<p>Let‚Äôs begin with an input sentence ‚ÄúLife is short, eat dessert first‚Äù. We need to create a sentence embedding before passing this input through the self-attention mechanism.</p>

<p>For the sake of simplicity, we‚Äôll only consider words in the input sentence, but in practice, most implementations would have a training dataset with many thousands of words.</p>

<p>To embed the words in our input sentence, we‚Äôll run through the following procedure:</p>
<ol>
  <li>Create a word-to-index dictionary of our input sentence after sorting the words alphabetically</li>
  <li>Map the words in the original unsorted sentence to their corresponding indices. Store the indexed unsorted input sentence as a PyTorch tensor for further numerical processing</li>
  <li>Map each word to a 16-dimensional vector via a PyTorch embedding layer</li>
</ol>

<p>Let‚Äôs go through each one, step-by-step.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<p><b>Step 1: Create a word-to-index dictionary of our input sentence after sorting the words alphabetically</b></p>

<p>Input:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sentence</span> <span class="o">=</span> <span class="s">"Life is short, eat dessert first"</span>

<span class="n">dc</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">sentence</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">','</span><span class="p">,</span><span class="s">''</span><span class="p">).</span><span class="n">split</span><span class="p">()))}</span>
<span class="k">print</span><span class="p">(</span><span class="n">dc</span><span class="p">)</span>
</code></pre></div></div>

<p>Output:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="s">'Life'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'dessert'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'eat'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">'first'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">'is'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">'short'</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
</code></pre></div></div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<p><b>Step 2: Map the words in the original unsorted sentence to their corresponding indices, then store the indexed unsorted input sentence as a PyTorch tensor for further numerical processing</b></p>

<p>Input:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">sentence_int</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span> <span class="p">[</span> <span class="n">dc</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">','</span><span class="p">,</span><span class="s">''</span><span class="p">).</span><span class="n">split</span><span class="p">()])</span>
<span class="k">print</span><span class="p">(</span><span class="n">sentence_int</span><span class="p">)</span>
</code></pre></div></div>

<p>Output:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</code></pre></div></div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<p><b>Step 3: Map each word to a 16-dimensional vector via a PyTorch embedding layer</b></p>

<p>Input:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>
<span class="n">embedded_sentence</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="n">sentence_int</span><span class="p">).</span><span class="n">detach</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">embedded_sentence</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">embedded_sentence</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p>Output:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span><span class="p">([[</span> <span class="mf">1.9269</span><span class="p">,</span>  <span class="mf">1.4873</span><span class="p">,</span>  <span class="mf">0.9007</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1055</span><span class="p">,</span>  <span class="mf">0.6784</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2345</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0431</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6047</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.7521</span><span class="p">,</span>  <span class="mf">1.6487</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3925</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4036</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7279</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5594</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7688</span><span class="p">,</span>  <span class="mf">0.7624</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.4451</span><span class="p">,</span>  <span class="mf">0.8564</span><span class="p">,</span>  <span class="mf">2.2181</span><span class="p">,</span>  <span class="mf">0.5232</span><span class="p">,</span>  <span class="mf">0.3466</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1973</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0546</span><span class="p">,</span>  <span class="mf">1.2780</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.1722</span><span class="p">,</span>  <span class="mf">0.5238</span><span class="p">,</span>  <span class="mf">0.0566</span><span class="p">,</span>  <span class="mf">0.4263</span><span class="p">,</span>  <span class="mf">0.5750</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6417</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7508</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0109</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3387</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3407</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5854</span><span class="p">,</span>  <span class="mf">0.5362</span><span class="p">,</span>  <span class="mf">0.5246</span><span class="p">,</span>  <span class="mf">1.1412</span><span class="p">,</span>  <span class="mf">0.0516</span><span class="p">,</span>
          <span class="mf">0.7440</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4816</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0495</span><span class="p">,</span>  <span class="mf">0.6039</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7223</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8278</span><span class="p">,</span>  <span class="mf">1.3347</span><span class="p">,</span>  <span class="mf">0.4835</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.3847</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8712</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2234</span><span class="p">,</span>  <span class="mf">1.7174</span><span class="p">,</span>  <span class="mf">0.3189</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4245</span><span class="p">,</span>  <span class="mf">0.3057</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7746</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.5576</span><span class="p">,</span>  <span class="mf">0.9956</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8798</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6011</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2742</span><span class="p">,</span>  <span class="mf">2.1228</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2347</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4879</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.6423</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1596</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4974</span><span class="p">,</span>  <span class="mf">0.4396</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7581</span><span class="p">,</span>  <span class="mf">1.0783</span><span class="p">,</span>  <span class="mf">0.8008</span><span class="p">,</span>  <span class="mf">1.6806</span><span class="p">,</span>
          <span class="mf">1.2791</span><span class="p">,</span>  <span class="mf">1.2964</span><span class="p">,</span>  <span class="mf">0.6105</span><span class="p">,</span>  <span class="mf">1.3347</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2316</span><span class="p">,</span>  <span class="mf">0.0418</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2516</span><span class="p">,</span>  <span class="mf">0.8599</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9138</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6581</span><span class="p">,</span>  <span class="mf">0.0780</span><span class="p">,</span>  <span class="mf">0.5258</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4880</span><span class="p">,</span>  <span class="mf">1.1914</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8140</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7360</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.4032</span><span class="p">,</span>  <span class="mf">0.0360</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0635</span><span class="p">,</span>  <span class="mf">0.6756</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0978</span><span class="p">,</span>  <span class="mf">1.8446</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1845</span><span class="p">,</span>  <span class="mf">1.3835</span><span class="p">]])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
</code></pre></div></div>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="defining-the-weight-matrices">Defining the Weight Matrices</h3>

<p>Recall that for a given input $X$, we pass it through the weight matrices $W_Q, W_K, W_V$ to obtain the Queries, Keys and Values matrices respectively.</p>

<p>The scaled dot-product self-attention mechanism utilizes these three weight matrices and optimizes them as model parameters during training.</p>

<p>Since we are computing the dot product between the Queries and Keys matrices later, we need each row in the two weight matrices to have the same number of elements. On the other hand, the number of elements in each row of the Values matrix is arbitrary.</p>

<p>So, we‚Äôll set $d_q = d_k = 24$ and $d_q$ = 28 arbitrarily, such that:</p>
<ul>
  <li>$W_K$ has dimensions $d \times d_K = 16 \times 24$</li>
  <li>$W_Q$ has dimensions $d \times d_Q = 16 \times 24$</li>
  <li>$W_V$ has dimensions $d \times d_V = 16 \times 28$</li>
</ul>

<p>(Given our X is of dimensions 6x16.)</p>

<p><b>We‚Äôll define the weight matrices like so:</b></p>

<p>Input:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">embedded_sentence</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">d_k</span><span class="p">,</span> <span class="n">d_q</span><span class="p">,</span> <span class="n">d_v</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">28</span>

<span class="n">W_K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">d_k</span><span class="p">))</span>
<span class="n">W_Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">d_q</span><span class="p">))</span>
<span class="n">W_V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">d_v</span><span class="p">))</span>
</code></pre></div></div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="computing-the-attention-weights-from-kq">Computing the Attention Weights from K,Q</h3>

<p>Now, we‚Äôll compute the unnormalized attention weights (also called attention scores) for the Keys, Queries and Values matrices.</p>

<p>We need to first compute the Q,K,V matrices via matrix multiplication between the weight matrices and the embedded vectors:</p>
<ul>
  <li>$K = XW_K$</li>
  <li>$Q = XW_Q$</li>
  <li>$V = XW_V$</li>
</ul>

<p>Input:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">K</span> <span class="o">=</span> <span class="n">embedded_sentence</span> <span class="o">@</span> <span class="n">W_K</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">embedded_sentence</span> <span class="o">@</span> <span class="n">W_Q</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">embedded_sentence</span> <span class="o">@</span> <span class="n">W_V</span>

<span class="k">print</span><span class="p">(</span><span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">Q</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">V</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p>Output:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">24</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">24</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
</code></pre></div></div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<p>Then, we will proceed to find the attention scores from the dot product of K and Q, followed by SoftMax scaling to obtain the attention weights.</p>

<p>Input:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">K</span> <span class="o">@</span> <span class="n">Q</span><span class="p">.</span><span class="n">T</span>
<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># dim = -1 ensures softmax is applied along each row
</span>
<span class="k">print</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span>
</code></pre></div></div>

<p>Output:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.4928e-02</span><span class="p">,</span> <span class="mf">2.5604e-05</span><span class="p">,</span> <span class="mf">5.2063e-04</span><span class="p">,</span> <span class="mf">9.8024e-01</span><span class="p">,</span> <span class="mf">5.9773e-10</span><span class="p">,</span> <span class="mf">4.2890e-03</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">4.9262e-20</span><span class="p">,</span> <span class="mf">5.3068e-10</span><span class="p">,</span> <span class="mf">1.7273e-17</span><span class="p">,</span> <span class="mf">2.0560e-22</span><span class="p">,</span> <span class="mf">1.0000e+00</span><span class="p">,</span> <span class="mf">1.1476e-16</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">4.8625e-02</span><span class="p">,</span> <span class="mf">1.8945e-04</span><span class="p">,</span> <span class="mf">4.8195e-03</span><span class="p">,</span> <span class="mf">9.3794e-01</span><span class="p">,</span> <span class="mf">3.6255e-06</span><span class="p">,</span> <span class="mf">8.4234e-03</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.1822e-02</span><span class="p">,</span> <span class="mf">8.9849e-16</span><span class="p">,</span> <span class="mf">5.7813e-07</span><span class="p">,</span> <span class="mf">9.8818e-01</span><span class="p">,</span> <span class="mf">3.4392e-29</span><span class="p">,</span> <span class="mf">2.2000e-07</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0000e+00</span><span class="p">,</span> <span class="mf">6.7095e-29</span><span class="p">,</span> <span class="mf">5.6052e-45</span><span class="p">,</span> <span class="mf">0.0000e+00</span><span class="p">,</span> <span class="mf">1.0000e+00</span><span class="p">,</span> <span class="mf">1.4013e-44</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">8.2369e-01</span><span class="p">,</span> <span class="mf">1.8583e-04</span><span class="p">,</span> <span class="mf">3.6337e-02</span><span class="p">,</span> <span class="mf">1.2967e-01</span><span class="p">,</span> <span class="mf">9.2257e-08</span><span class="p">,</span> <span class="mf">1.0115e-02</span><span class="p">]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">SoftmaxBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="calculating-attention-weighted-context-vectors">Calculating Attention-Weighted Context Vectors</h3>

<p>The subsequent step is to use the Attention Weights earlier, to compute the weighted embedding vectors from V.</p>

<p>Input:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">context_vectors</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="o">@</span> <span class="n">V</span>

<span class="k">print</span><span class="p">(</span><span class="n">context_vectors</span><span class="p">)</span>
</code></pre></div></div>

<p>Output:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.4015</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6157</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1565</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4481</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2659</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0132</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0527</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8838</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8617</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6299</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8879</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.8774</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4376</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0597</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2499</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.1146</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3537</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7522</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.4342</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4208</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7473</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5230</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6547</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6916</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.2874</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0094</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0400</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9443</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.5695</span><span class="p">,</span>  <span class="mf">5.0923</span><span class="p">,</span>  <span class="mf">3.9690</span><span class="p">,</span>  <span class="mf">4.2990</span><span class="p">,</span>  <span class="mf">4.3684</span><span class="p">,</span>  <span class="mf">3.0220</span><span class="p">,</span>  <span class="mf">4.8688</span><span class="p">,</span>  <span class="mf">2.8423</span><span class="p">,</span>
          <span class="mf">4.6958</span><span class="p">,</span>  <span class="mf">3.9716</span><span class="p">,</span>  <span class="mf">5.8949</span><span class="p">,</span>  <span class="mf">3.6307</span><span class="p">,</span>  <span class="mf">2.4154</span><span class="p">,</span>  <span class="mf">3.7492</span><span class="p">,</span>  <span class="mf">4.7356</span><span class="p">,</span>  <span class="mf">6.9815</span><span class="p">,</span>
          <span class="mf">4.3885</span><span class="p">,</span>  <span class="mf">3.3385</span><span class="p">,</span>  <span class="mf">6.3139</span><span class="p">,</span>  <span class="mf">5.7449</span><span class="p">,</span>  <span class="mf">1.7411</span><span class="p">,</span>  <span class="mf">6.5813</span><span class="p">,</span>  <span class="mf">5.1148</span><span class="p">,</span>  <span class="mf">5.8224</span><span class="p">,</span>
          <span class="mf">3.6271</span><span class="p">,</span>  <span class="mf">3.8196</span><span class="p">,</span>  <span class="mf">4.8301</span><span class="p">,</span>  <span class="mf">5.6469</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.3355</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4879</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0844</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3353</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2875</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0540</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0939</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8983</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.9252</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5819</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9411</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7367</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3410</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1087</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2195</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9842</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3826</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7398</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2849</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5222</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6938</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3847</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5709</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5644</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.2504</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9646</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9809</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9704</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.4174</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6358</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1691</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4719</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2714</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0124</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0521</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8812</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8608</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6457</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8887</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9022</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4493</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0604</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2594</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.1403</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3527</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7580</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.4579</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4195</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7627</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5455</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6765</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7127</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.2983</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0249</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0482</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9548</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.5695</span><span class="p">,</span>  <span class="mf">5.0923</span><span class="p">,</span>  <span class="mf">3.9690</span><span class="p">,</span>  <span class="mf">4.2990</span><span class="p">,</span>  <span class="mf">4.3684</span><span class="p">,</span>  <span class="mf">3.0220</span><span class="p">,</span>  <span class="mf">4.8688</span><span class="p">,</span>  <span class="mf">2.8423</span><span class="p">,</span>
          <span class="mf">4.6958</span><span class="p">,</span>  <span class="mf">3.9716</span><span class="p">,</span>  <span class="mf">5.8949</span><span class="p">,</span>  <span class="mf">3.6307</span><span class="p">,</span>  <span class="mf">2.4154</span><span class="p">,</span>  <span class="mf">3.7492</span><span class="p">,</span>  <span class="mf">4.7356</span><span class="p">,</span>  <span class="mf">6.9815</span><span class="p">,</span>
          <span class="mf">4.3885</span><span class="p">,</span>  <span class="mf">3.3385</span><span class="p">,</span>  <span class="mf">6.3139</span><span class="p">,</span>  <span class="mf">5.7449</span><span class="p">,</span>  <span class="mf">1.7411</span><span class="p">,</span>  <span class="mf">6.5813</span><span class="p">,</span>  <span class="mf">5.1148</span><span class="p">,</span>  <span class="mf">5.8224</span><span class="p">,</span>
          <span class="mf">3.6271</span><span class="p">,</span>  <span class="mf">3.8196</span><span class="p">,</span>  <span class="mf">4.8301</span><span class="p">,</span>  <span class="mf">5.6469</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2417</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0091</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7194</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3319</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9164</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1096</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0993</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2251</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">2.5139</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9369</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3374</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0260</span><span class="p">,</span>  <span class="mf">0.7404</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4244</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8316</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5802</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.1297</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7487</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5116</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0171</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7168</span><span class="p">,</span>  <span class="mf">0.3634</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1516</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0040</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.5149</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2408</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8115</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9363</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="summary-packaging-the-self-attention-mechanism-in-a-pytorch-module">Summary: Packaging the self-attention mechanism in a PyTorch module</h3>

<p>Let‚Äôs bring all the code together as a module, such that calling the forward pass would return the context vectors.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_q</span><span class="p">,</span> <span class="n">d_v</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">d</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_k</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_q</span> <span class="o">=</span> <span class="n">d_q</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_v</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">W_K</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">d_k</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_Q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">d_q</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_V</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">d_v</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_K</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_Q</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_V</span>

        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context_vector</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="o">@</span> <span class="n">V</span>

        <span class="k">return</span> <span class="n">context_vector</span> 

</code></pre></div></div>

<p>Okay! That‚Äôs the Scaled Dot-Product Self-Attention Mechanism implemented in code.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>¬†</mtext><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex"> ~ \\ </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace nobreak">¬†</span></span><span class="mspace newline"></span></span></span></span></p>

<h3 id="2-references">2. References</h3>

<p>This project post was primarily an exercise in re-implementing and modifying Sebastian Raschka‚Äôs excellent blog post on the same topic.<a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html" target="_blank">excellent blog post on the same topic</a></p>

<p>I also found this</p>
:ET